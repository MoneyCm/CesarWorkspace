import streamlit as st
import os
from db.session import SessionLocal
from db.models import Question
from core.generators.llm import LLMGenerator
from core.dedupe import compute_hash
from ui_utils import load_css, render_header

import pypdf
import io

st.set_page_config(page_title="Generador IA | DIAN Sim", page_icon="ü§ñ", layout="wide")
load_css()
render_header(title="Generador de Preguntas con IA", subtitle="Crea material de estudio a partir de documentos")

st.markdown("""
<div class="dian-card">
    Aqu√≠ puedes usar inteligencia artificial para crear preguntas a partir de tus propios documentos.
    <br><b>Nota:</b> Necesitas una API Key de OpenAI o Google Gemini.
</div>
""", unsafe_allow_html=True)

with st.expander("üîê Configuraci√≥n de API Key", expanded=True):
    provider = st.selectbox("Proveedor", ["OpenAI", "Gemini"])
    api_key_env = os.getenv(f"{provider.upper()}_API_KEY")
    api_key = st.text_input(f"API Key {provider}", value=api_key_env if api_key_env else "", type="password")
    
    st.caption("No guardamos tu llave. Se usa solo para esta sesi√≥n.")

st.divider()

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("1. Origen del Contenido")
    
    tab_text, tab_file = st.tabs(["üìã Pegar Texto", "üìÇ Subir Archivo"])
    
    source_text = ""
    
    with tab_text:
        pasted_text = st.text_area("Pega aqu√≠ el art√≠culo o ley:", height=300)
        if pasted_text:
            source_text = pasted_text
            
    with tab_file:
        uploaded_file = st.file_uploader("Sube un documento (PDF, TXT)", type=["pdf", "txt"])
        if uploaded_file:
            try:
                if uploaded_file.type == "application/pdf":
                    reader = pypdf.PdfReader(uploaded_file)
                    extracted = []
                    for page in reader.pages:
                        extracted.append(page.extract_text())
                    source_text = "\n".join(extracted)
                    st.success(f"PDF cargado: {len(reader.pages)} p√°ginas le√≠das.")
                else:
                    # TXT
                    source_text = uploaded_file.read().decode("utf-8")
                    st.success("Archivo de texto cargado.")
            except Exception as e:
                st.error(f"Error leyendo archivo: {e}")

    st.caption(f"Caracteres detectados: {len(source_text)}")
    
    # Pre-fill from session state if available (Workflow from New Simulation)
    default_topic = st.session_state.get("ai_default_topic", "Gestor II")
    
    # Check if we should auto-fill source text from session
    if "ai_default_text" in st.session_state and not source_text:
        source_text = st.session_state["ai_default_text"]
        st.info("‚ÑπÔ∏è Se ha cargado el texto del perfil seleccionado autom√°ticamente.")
        # Clear it so it doesn't stick forever? Maybe keep it.
    
    custom_topic = st.text_input("Etiqueta / Tema para estas preguntas (Ej: Gestor II)", value=default_topic)
    
    num_q = st.slider("Cantidad de preguntas a generar", 1, 10, 3)
    
    generate_btn = st.button("‚ú® Generar Preguntas", disabled=(not source_text or not api_key), type="primary")

if generate_btn:
    with st.spinner("Analizando texto y creando preguntas... (Esto puede tardar unos segundos)"):
        try:
            generator = LLMGenerator(provider, api_key)
            results = generator.generate_from_text(source_text, num_q)
            
            # Apply Custom Topic Override
            if results and custom_topic.strip():
                for q in results:
                    q['topic'] = custom_topic.strip()
            
            st.session_state["generated_questions"] = results
            
            if not results:
                st.error("No se pudieron generar preguntas. Revisa tu API Key o el formato del texto.")
            else:
                st.success(f"¬°{len(results)} preguntas generadas!")
        except Exception as e:
            st.error(f"Error: {str(e)}")

# Create DB Session
db = SessionLocal()

with col2:
    st.subheader("2. Revisar y Guardar")
    if "generated_questions" in st.session_state and st.session_state["generated_questions"]:
        candidates = st.session_state["generated_questions"]
        
        indices_to_save = []
        
        for i, q in enumerate(candidates):
            with st.container(border=True):
                st.write(f"**P{i+1}: {q['stem']}**")
                st.caption(f"{q['track']} | {q['topic']}")
                
                # Show Options
                ops = q.get('options_json', {})
                if ops:
                    for k, v in ops.items():
                        st.text(f"{k}) {v}")
                
                st.info(f"Respuesta Correcta: {q['correct_key']}")
                
                if q.get('rationale'):
                    st.caption(f"**Justificaci√≥n:** {q['rationale']}")
                
                # Check Duplicates
                exists = db.query(Question).filter_by(hash_norm=q['hash_norm']).first()
                if exists:
                    st.warning("‚ö†Ô∏è Pregunta muy similar ya existe en el banco.")
                else:
                    if st.checkbox("Guardar esta pregunta", key=f"save_{i}", value=True):
                        indices_to_save.append(i)
        
        if st.button("üíæ Guardar Seleccionadas en Banco", type="primary"):
            saved_count = 0
            for i in indices_to_save:
                data = candidates[i]
                # Re-check existence just in case
                if not db.query(Question).filter_by(hash_norm=data['hash_norm']).first():
                    new_q = Question(**data)
                    db.add(new_q)
                    saved_count += 1
            
            db.commit()
            st.balloons()
            st.success(f"Se guardaron {saved_count} preguntas nuevas.")
            # Clear session
            del st.session_state["generated_questions"]
            st.rerun()
            
    else:
        st.info("Las preguntas generadas aparecer√°n aqu√≠ para tu revisi√≥n.")

db.close()
